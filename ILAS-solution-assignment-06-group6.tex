	% HMC Math dept HW template example
	% v0.04 by Eric J. Malm, 10 Mar 2005
	\documentclass[10pt,a4paper,boxed]{hmcpset}

	% set 1-inch margins in the document
	% \usepackage[margin=1in]{geometry}
	\usepackage{enumerate}
	\usepackage{todonotes}
	\usepackage{tikz}
	\usetikzlibrary{positioning}
	\usepackage{subfig} % subfigures in figures.	
	\usepackage{pgfplots}
	\usepackage{amsmath}
	\usepackage{amsfonts}
	\usepackage{amssymb}
	
	%% work around for subfig and asy environment
	\makeatletter
	\newsavebox{\sfe@box}
	\newenvironment{subfloatenv}[2]{%
	\def\sfe@caption{#1}%
	\def\sfe@label{#2}%
	\setbox\sfe@box\hbox\bgroup\color@setgroup}%
	{\color@endgroup\egroup\subfloat[\sfe@caption]%
	{\usebox{\sfe@box}\label{\sfe@label}}}
	\makeatother

	% include this if you want to import graphics files with /includegraphics
	\usepackage{graphicx}

	\renewcommand*{\familydefault}{\sfdefault}
	\newcommand{\vect}[1]{\mathbf{#1}}
	\newcommand{\Vor}[1]{\textrm{Vor}(#1)}
	\DeclareMathOperator{\gain}{Gain}
	\DeclareMathOperator{\entropy}{H}
	\DeclareMathOperator{\prob}{P}

	\tikzset{node distance=2cm, inner/.style={draw,circle}, leaf/.style={draw,rectangle}}

	\usepackage{hyperref}

	% info for header block in upper right hand corner
	\name{Group 6: Timm Behner, Philipp Bruckschen, Patrick Kaster, Markus Schwalb}
	\class{MA-INF 4111 - Intelligent Learning and Analysis Systems: Machine Learning}
	\assignment{Exercise Sheet 4}
	% \duedate{09/03/2004}

	\begin{document}
	
		\begin{problem}[1. Confidence Intervals]
		\end{problem}

		\begin{solution}		
			\begin{enumerate}[(a)]
				\item given: $\textrm{error}_S(h) = \frac{10}{65}, n=65$
					\begin{align*}
						\textrm{error}_{\textrm{D}}^{95\%}(h) & \leq \textrm{error}_S(h) + z_N \sqrt{\frac{\textrm{error}_S(h)(1-\textrm{error}_S(h))}{n}}\\
															  & = \frac{10}{65} + 1.96 \sqrt{\frac{\frac{10}{65}(1-\frac{10}{65})}{65}}\\
															  & \thickapprox 0.24
					\end{align*}
					
				\item 
					\begin{align*}
						\textrm{error}_{\textrm{D}}^{90\%}(h) & \leq \frac{10}{65} + 1.64 					\sqrt{\frac{\frac{10}{65}(1-\frac{10}{65})}{65}}\\
															  & \thickapprox 0.23
					\end{align*}
									 			
			\end{enumerate}
			
		\end{solution}		

		\begin{problem}[2. Sample Size]
		\end{problem}
		\begin{solution}
			given: $\textrm{error}_D(h) \in [0.2, 0.6] \Rightarrow \sigma = 0.2$\\
			wanted: bound on error $E<0.1$
			\begin{align*}
				E & = z_N \frac{\sigma}{\sqrt{n}} \Rightarrow \\
				n & = \left( z_N \frac{\sigma}{E} \right)^2
			\end{align*}
			\begin{align*}
				n & = \left( 1.96 \frac{0.2}{0.1} \right)^2 \\
				  & = \left\lceil 15.3664 \right\rceil = 16
			\end{align*}
		\end{solution}
		
		\begin{problem}[4. AUC]
				\end{problem}
		
				\begin{solution}		
					\begin{enumerate}[(a)]
						\item 
							\begin{align*}
								\textrm{Accuracy}_S(h) & = \frac{\textrm{\# Correctly Classified}}{\textrm{\#Test Examples}}\\
												       & = \frac{990}{100} = 0.99
							\end{align*}
					\end{enumerate}
				\end{solution}		
\begin{problem}[5. AUC]
\end{problem}
\begin{solution}
Use probability based ranking as described in the lecture. Assume that for every sample $x\in S$ there is a teacher output classifying $x$ to be a positive or negative sample. Define $H^+$ to be the half-space that contains the most positive samples and $H^-$ to be the other half-space. If both sides have an equal number of positive samples then choose one side to be $H^+$ at random. Now the probability that a sample $x\in S$ is positive is $p^+(x)=\frac{^+ n_S^{H^i}}{n_S^{H^i}}$ where $i\in\{+,-\}$, $i$ is the half-space where sample x is located in, $^+ n_S^{H^i}$ is the number of all samples in $H^i$ that are ranked positive and $n_S^{H^i}$ is the number of all samples in $H^i$. Obviously this would only lead to 2 ranks and would be quite a poor ranking.\\
A better ranking would be to consider the distance of $x$ to the hyperplane $H$. The further away a point $x$ in $H^+$ is away from $H$, the better it is ranked (one hopes that the further away a point is from the border the more sure one can be that is in the correct half-space). On the other side a point $x$ in $H^-$ is ranked higher the closer it is to $H$ (here one hopes that a misclassified sample will be close to the border rather than being far away on the wrong side). Consider the distances between points $x$ and the hyperplane $H$ to be between $x$ and a point on the hyperplane that yields the smallest distance in the euclidean distance measure. In general this will lead to more ranks than 2. (unless all points are located equally far away from either side of $H$).
\end{solution}
	\end{document}
